# 6 学習に関するテクニック

## 6.3 Batch Normalization

図6-19：いくつか（右上二つ）は Batch Normalization が無い方が学習が進んでいる。ここでの主題は、「多くの場合において、Batch Normalization が有効に働く」である。

### 参考
* [Understanding the backward pass through Batch Normalization Layer](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)
* [Batch Normalizationによる学習の効率化](http://kdog08.hatenablog.com/entry/2017/06/20/080418)
* [Batch Normalization の理解](https://qiita.com/t-tkd3a/items/14950dbf55f7a3095600)
* 
