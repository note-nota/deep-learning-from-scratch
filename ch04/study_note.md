# 4 ニューラルネットワークの学習

## 4.3 数値微分

### 4.3.1 微分
`h = 10e-50` に対して **丸め誤差** の表記はある。また微分に対しての **前方差分・中央差分** に対する考察もなされている。

　ここで、**桁落ち** に関して考察する。微分の際には、ゼロ極限を考えるため比較的にごく近い範囲での差分計算を行うことになるため、桁落ちが発生しやすく誤差要因となる。

　そのため、対策として既知の関数形であれば、差分計算が起きない、あるいは近しい値の差分計算にならないように変形することが有効である。

cf.) [[Qiita] 桁の落ちない話](https://qiita.com/azapen6/items/fd03c9a8a29a00bb4626#%E6%A1%81%E8%90%BD%E3%81%A1%E3%81%AE%E4%BE%8B)

## 4.5 学習アルゴリズムの実装

### 4.5.1 ２層ニューラルネットワークのクラス
　`numerical_gradient` と `gradient` があり、「先取りして〜」とある。実際に MacBook Pro 13-inch (2018) で `numerical_gradient` を試したが、全く計算が進んでいる様子がなく、２時間で諦めた事を言い添えておく。
